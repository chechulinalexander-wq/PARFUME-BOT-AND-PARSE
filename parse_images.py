import sqlite3
import cloudscraper
from bs4 import BeautifulSoup
import sys
import io
import time
import os
import json
from urllib.parse import urljoin
import hashlib

# –§–∏–∫—Å –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows –∫–æ–Ω—Å–æ–ª–∏
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

def get_news_without_images():
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    conn = sqlite3.connect('fragrantica_news.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT id, title, url FROM news WHERE images IS NULL')
    news_list = cursor.fetchall()
    
    conn.close()
    return news_list

def download_image(url, save_dir='images'):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
    try:
        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL (–∏—Å–ø–æ–ª—å–∑—É–µ–º hash –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏)
        url_hash = hashlib.md5(url.encode()).hexdigest()
        extension = url.split('.')[-1].split('?')[0][:4]  # –ë–µ—Ä–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        if extension.lower() not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
            extension = 'jpg'
        
        filename = f"{url_hash}.{extension}"
        filepath = os.path.join(save_dir, filename)
        
        # –ï—Å–ª–∏ —É–∂–µ —Å–∫–∞—á–∞–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if os.path.exists(filepath):
            return filepath
        
        # –°–∫–∞—á–∏–≤–∞–µ–º
        scraper = cloudscraper.create_scraper()
        response = scraper.get(url, timeout=30, stream=True)
        response.raise_for_status()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return filepath
        
    except Exception as e:
        print(f"    ‚úó –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url[:50]}: {e}")
        return None

def parse_images_from_news(url):
    """–ü–∞—Ä—Å–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–∏"""
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'mobile': False
        }
    )
    
    try:
        response = scraper.get(url, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π div —Å –Ω–æ–≤–æ—Å—Ç—å—é
        card_div = soup.find('div', class_='card', style=lambda value: value and 'width: 100%' in value and 'position: relative' in value)
        
        if not card_div:
            card_div = soup.find('div', class_='card')
        
        if not card_div:
            return []
        
        # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = []
        for img in card_div.find_all('img'):
            img_url = img.get('src')
            if img_url:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                img_url = urljoin(url, img_url)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏–∫–æ–Ω–∫–∏ –∏ —Ç.–¥.)
                width = img.get('width', '')
                if width and width.isdigit() and int(width) < 100:
                    continue
                
                images.append(img_url)
        
        return images
        
    except Exception as e:
        print(f"  ‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return []

def save_images_to_db(news_id, image_paths):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –ë–î"""
    conn = sqlite3.connect('fragrantica_news.db')
    cursor = conn.cursor()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSON
    images_json = json.dumps(image_paths, ensure_ascii=False)
    cursor.execute('UPDATE news SET images = ? WHERE id = ?', (images_json, news_id))
    
    conn.commit()
    conn.close()

def process_news_images(news_id, title, url):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
    print(f"\nüì∞ {title[:50]}...")
    print(f"  üîó {url}")
    
    # –ü–∞—Ä—Å–∏–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    print("  üîç –ò—â—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    image_urls = parse_images_from_news(url)
    
    if not image_urls:
        print("  ‚äò –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        save_images_to_db(news_id, [])
        return 0
    
    print(f"  üì∏ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_urls)}")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    downloaded = []
    for i, img_url in enumerate(image_urls, 1):
        print(f"  [{i}/{len(image_urls)}] –°–∫–∞—á–∏–≤–∞—é: {img_url[:60]}...")
        filepath = download_image(img_url)
        if filepath:
            downloaded.append(filepath)
            print(f"    ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
        time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    save_images_to_db(news_id, downloaded)
    print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {len(downloaded)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return len(downloaded)

def main():
    print("=== –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π ===\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏
    if not os.path.exists('images'):
        print("‚úó –ü–∞–ø–∫–∞ images –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print("  –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python add_images_column.py")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    news_list = get_news_without_images()
    
    if not news_list:
        print("‚úì –£ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —É–∂–µ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        return
    
    print(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(news_list)}\n")
    
    total_images = 0
    processed = 0
    
    for news_id, title, url in news_list:
        processed += 1
        print(f"\n{'='*80}")
        print(f"[{processed}/{len(news_list)}]")
        
        count = process_news_images(news_id, title, url)
        total_images += count
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏
        if processed < len(news_list):
            time.sleep(2)
    
    print(f"\n{'='*80}")
    print("=== –†–µ–∑—É–ª—å—Ç–∞—Ç ===")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {processed}")
    print(f"–°–∫–∞—á–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}")
    print(f"–°—Ä–µ–¥–Ω–µ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {total_images/processed:.1f}")
    print(f"{'='*80}\n")

if __name__ == '__main__':
    main()



